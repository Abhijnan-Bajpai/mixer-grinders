{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bilingual-Sentiment_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBtRMbgGhg4m"
      },
      "source": [
        "#Mounting the Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EByI-sFKW_8U",
        "outputId": "b72442cc-4768-4960-b751-0e6052cabe89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkHDH2URiBRh"
      },
      "source": [
        "#Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw58JuYGXrIf"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras import backend as K\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Convolution1D, MaxPooling1D\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import keras.backend as K\n",
        "from keras.models import model_from_json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMT93yr2iJYs"
      },
      "source": [
        "#Loading pure English dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "J1V9czr4XJkp",
        "outputId": "aff080b1-3851-4a74-c7f2-cb88cece691a"
      },
      "source": [
        "data_source = '/content/drive/MyDrive/Data/Tweets.csv'\n",
        "train_df = pd.read_csv(data_source, header=None)\n",
        "train_df = train_df.iloc[1:]\n",
        "train_df.head()\n",
        "# len(train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0                                                  1\n",
              "1  1                @VirginAmerica What @dhepburn said.\n",
              "2  2  @VirginAmerica plus you've added commercials t...\n",
              "3  1  @VirginAmerica I didn't today... Must mean I n...\n",
              "4  0  @VirginAmerica it's really aggressive to blast...\n",
              "5  0  @VirginAmerica and it's a really big bad thing..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxm-856QjutY"
      },
      "source": [
        "#Preprocessing the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Q-kJaVXNoG"
      },
      "source": [
        "train_df[1] = train_df[1].str.lower()\n",
        "train_df[1] = train_df[1].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "train_df[1]= train_df[1].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "train_df[1] = train_df[1] .apply(lambda x: re.sub(r'{link}', '', x))\n",
        "train_df[1]= train_df[1].apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
        "train_df[1]=train_df[1] .apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "train_df[1] = train_df[1].apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
        "l = [\"virginamerica\",\"united\",\"jetblue\",\"southwestair\",\"usairways\",\"americanair\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzQ1k7PKXQeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11a9c3f-2bdb-473c-88d8-0697fee10783"
      },
      "source": [
        "texts = train_df[1].values # texts contrain all setences\n",
        "print(len(texts))\n",
        "print(texts[:2])\n",
        "for i in range(len(texts)):\n",
        "  for x in l:\n",
        "    if x in texts[i]:\n",
        "      texts[i] = texts[i].replace(x,\"\")\n",
        "print(len(texts))\n",
        "print(texts[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14640\n",
            "['virginamerica what dhepburn said'\n",
            " \"virginamerica plus you've added commercials to the experience tacky\"]\n",
            "14640\n",
            "[' what dhepburn said'\n",
            " \" plus you've added commercials to the experience tacky\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW5TmF6gj1xM"
      },
      "source": [
        "#Loading the Hinglish Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hl3VCAYXVhl",
        "outputId": "91d9d48c-c66f-4f31-b8eb-9132ce487452"
      },
      "source": [
        "data_source2 = '/content/drive/MyDrive/Data/Hindi_english_dataset.csv'\n",
        "train_df2 = pd.read_csv(data_source2, header=None)\n",
        "train_df2 = train_df2.iloc[1:]\n",
        "train_df2.head()\n",
        "len(train_df2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6357"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPF5Ey4ImlNF"
      },
      "source": [
        "#Preprocessing the loaded dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kixA4Y0dm84J"
      },
      "source": [
        "##Regex for removing unnecessary text from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwJCPja1YwjS"
      },
      "source": [
        "train_df2[0] = train_df2[0].str.lower()\n",
        "train_df2[0] = train_df2[0].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "train_df2[0]= train_df2[0].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "train_df2[0] = train_df2[0] .apply(lambda x: re.sub(r'{link}', '', x))\n",
        "train_df2[0]= train_df2[0].apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
        "train_df2[0]=train_df2[0] .apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "train_df2[0] = train_df2[0].apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "GmJdB67hY79w",
        "outputId": "35436c32-d745-40b3-86cc-60ce56fba83f"
      },
      "source": [
        "texts2 = train_df2[0].values # texts contrain all setences\n",
        "#texts = [s.lower() for s in texts] # convert to lower case \n",
        "print(len(texts2))\n",
        "print(texts2[:2])\n",
        "textlab =  train_df2[1].values\n",
        "for i in range(len(textlab)):\n",
        "  textlab[i]=int(textlab[i])\n",
        "train_df2[2]=textlab\n",
        "train_df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6357\n",
            "['jab tak kohli captain se aur shastri coach se dismissed nhi honge tab tak koi big tournament jeetenge hi nhi'\n",
            " 'really i  agree to you viru paaji ab ravi shastri ko istifa dena chaiye']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jab tak kohli captain se aur shastri coach se ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>really i  agree to you viru paaji ab ravi shas...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aur  ke players ki mentality aur techniques s...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kewal ravi shashtri ko bhagao indian team phir...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>jab rohitsharma  run bana raha tha to ye godi ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6353</th>\n",
              "      <td>hindu muslim mien kon sa payar ha jo akbar bha...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6354</th>\n",
              "      <td>hindu muslim ka asli dange toye mc modi kar ra...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6355</th>\n",
              "      <td>are randi wali mother chood  modiyaa kee  khil...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6356</th>\n",
              "      <td>modi ji ne jo aatang danga cheating karke indi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6357</th>\n",
              "      <td>bjp vale ki gand jali aysa to sab bolte miya b...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6357 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0  1  2\n",
              "1     jab tak kohli captain se aur shastri coach se ...  0  0\n",
              "2     really i  agree to you viru paaji ab ravi shas...  1  1\n",
              "3      aur  ke players ki mentality aur techniques s...  1  1\n",
              "4     kewal ravi shashtri ko bhagao indian team phir...  0  0\n",
              "5     jab rohitsharma  run bana raha tha to ye godi ...  0  0\n",
              "...                                                 ... .. ..\n",
              "6353  hindu muslim mien kon sa payar ha jo akbar bha...  1  1\n",
              "6354  hindu muslim ka asli dange toye mc modi kar ra...  1  1\n",
              "6355  are randi wali mother chood  modiyaa kee  khil...  0  0\n",
              "6356  modi ji ne jo aatang danga cheating karke indi...  1  1\n",
              "6357  bjp vale ki gand jali aysa to sab bolte miya b...  0  0\n",
              "\n",
              "[6357 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPy7E19mtGx"
      },
      "source": [
        "##Selecting positive and negative sentiment sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcrkqcflnODY"
      },
      "source": [
        "###From pure English dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKTXTm3LZ_fG"
      },
      "source": [
        "positive_Eng = list(train_df[train_df[0] == '2'][1])\n",
        "negative_Eng = list(train_df[train_df[0] == '0'][1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4R8IznnQwO"
      },
      "source": [
        "###From Hinglish dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ABM4VgBdknU"
      },
      "source": [
        "positive_HiEng = list(train_df2[train_df2[2] == 2][0])\n",
        "negative_HiEng = list(train_df2[train_df2[2] == 0][0])\n",
        "# len(train_df2[train_df2[2] == 1][0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krNhvrX9p2oA",
        "outputId": "8bf02bf6-2fbe-47a4-ef12-7114781d6f6d"
      },
      "source": [
        "print(negative_HiEng[:2])\n",
        "l=list(train_df[0])\n",
        "type(l[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jab tak kohli captain se aur shastri coach se dismissed nhi honge tab tak koi big tournament jeetenge hi nhi', 'kewal ravi shashtri ko bhagao indian team phir se bare tournament jeetne lagegi dhanyawad ']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzWpMw6Fnf-0"
      },
      "source": [
        "##Selecting the same number of sentences for each sentiment from both the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvh-4RgRamqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b076eaf-d205-4642-d27f-10afb78dbb6d"
      },
      "source": [
        "Pos_eng_6k= positive_Eng[:2000]\n",
        "neg_eng_6k = negative_Eng[:2000]\n",
        "Pos_hieng_6k = positive_HiEng[:2000]\n",
        "neg_hieng_6k = negative_HiEng[:2000]\n",
        "len(negative_HiEng)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1637"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GshsI5qnu8W"
      },
      "source": [
        "Text1: Anchor sentence from pure english <br>\n",
        "Text2: Pairing a permutation of sentiments with the anchor <br>\n",
        "Label: 0: If both belong to opposite sentiment, 1: If both belong to the same sentiment\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q1_4El_bIRo"
      },
      "source": [
        "new_df = pd.DataFrame(columns=['text1', 'text2', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeO77JaRbS_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c442a3-c444-435e-a8e0-ce05202f8a4f"
      },
      "source": [
        "for data in Pos_eng_6k:   #Taking anchor as Positive\n",
        "  data1 = data\n",
        "  data2 = random.choice(Pos_hieng_6k)\n",
        "  data3 = random.choice(neg_hieng_6k)\n",
        "\n",
        "  new_df.loc[len(new_df)] = [data1, data2, 1]\n",
        "  new_df.loc[len(new_df)] = [data1, data3, 0]\n",
        "new_df[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>modi g ke jajbe ko koti koti naman</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>do chutiya ek saath great moment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
              "      <td>two ligands jay hind</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
              "      <td>chutiya bna rahey ho kya bhai:p boley to lucky...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>well i didn'tbut now i do :-d</td>\n",
              "      <td>aaj dil khush hua iska sabse bada faisla mil gya</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>well i didn'tbut now i do :-d</td>\n",
              "      <td>kaheytey ho ki muslim fans pic nahi dekhey to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>it was amazing and arrived an hour early you'...</td>\n",
              "      <td>jaanma main bol rahi hu kitum mere twits dekho :/</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>it was amazing and arrived an hour early you'...</td>\n",
              "      <td>jb  over me  runs chahiye or dhoni  balls khel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>i  pretty graphics so much better than minima...</td>\n",
              "      <td>isse pta chalta hai caa nrc or population cont...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>i  pretty graphics so much better than minima...</td>\n",
              "      <td>nahi to ye mat soch ke tu fanna nahi hoga ye k...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               text1  ... label\n",
              "0   plus you've added commercials to the experien...  ...     1\n",
              "1   plus you've added commercials to the experien...  ...     0\n",
              "2   yes nearly every time i fly vx this ear worm ...  ...     1\n",
              "3   yes nearly every time i fly vx this ear worm ...  ...     0\n",
              "4                      well i didn'tbut now i do :-d  ...     1\n",
              "5                      well i didn'tbut now i do :-d  ...     0\n",
              "6   it was amazing and arrived an hour early you'...  ...     1\n",
              "7   it was amazing and arrived an hour early you'...  ...     0\n",
              "8   i  pretty graphics so much better than minima...  ...     1\n",
              "9   i  pretty graphics so much better than minima...  ...     0\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ouvWYAcPBd"
      },
      "source": [
        "for data in neg_eng_6k:   #Taking anchor as Negative and appending at aend of df2\n",
        "  data1 = data\n",
        "  data2 = random.choice(neg_hieng_6k)\n",
        "  data3 = random.choice(Pos_hieng_6k)\n",
        "\n",
        "  new_df.loc[len(new_df)] = [data1, data2, 1]\n",
        "  new_df.loc[len(new_df)] = [data1, data3, 0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF1iF83XdMMu",
        "outputId": "d63caf13-8ab2-432c-a231-034166488d00"
      },
      "source": [
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNPBLXN7eRwC",
        "outputId": "fd184ac1-6430-41c2-e9f8-1ebd1c4b13f7"
      },
      "source": [
        "display(new_df.sample(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7135</th>\n",
              "      <td>we are delayed in san pedro belize we are sch...</td>\n",
              "      <td>wah sir g ab desh sahi hatho me gya h apko dek...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2425</th>\n",
              "      <td>thanks do yall expect to be operational tomor...</td>\n",
              "      <td>aur sunlo humko reply krne waalon hum to usko ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6603</th>\n",
              "      <td>i bought a ticket with a price that was publi...</td>\n",
              "      <td>superb salman bhai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7091</th>\n",
              "      <td>stuck in dc trying to get to denver the engin...</td>\n",
              "      <td>yess bhaaijaan hum aapke sath hai hamesha kuch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5344</th>\n",
              "      <td>is the worst worst reservation policies worst...</td>\n",
              "      <td>finally nirbhaya got justice rip nirbhayauun d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text1  ... label\n",
              "7135   we are delayed in san pedro belize we are sch...  ...     0\n",
              "2425   thanks do yall expect to be operational tomor...  ...     0\n",
              "6603   i bought a ticket with a price that was publi...  ...     0\n",
              "7091   stuck in dc trying to get to denver the engin...  ...     0\n",
              "5344   is the worst worst reservation policies worst...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCGwEclVovj8"
      },
      "source": [
        "##Splitting the dataset into training, testing and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XCRvgeSeelp"
      },
      "source": [
        "X_temp, X_test, y_temp, y_test = train_test_split(new_df[['text1', 'text2']], new_df['label'], test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp[['text1', 'text2']], y_temp, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXvVOLpLgc4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8944919-e020-4a75-edd0-75951adc0c5b"
      },
      "source": [
        "new_df['text'] = new_df[['text1', 'text2']].apply(lambda x: str(x[0])+\" \"+str(x[1]), axis=1)\n",
        "new_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>modi g ke jajbe ko koti koti naman</td>\n",
              "      <td>1</td>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>do chutiya ek saath great moment</td>\n",
              "      <td>0</td>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
              "      <td>two ligands jay hind</td>\n",
              "      <td>1</td>\n",
              "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
              "      <td>chutiya bna rahey ho kya bhai:p boley to lucky...</td>\n",
              "      <td>0</td>\n",
              "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>well i didn'tbut now i do :-d</td>\n",
              "      <td>aaj dil khush hua iska sabse bada faisla mil gya</td>\n",
              "      <td>1</td>\n",
              "      <td>well i didn'tbut now i do :-d aaj dil khush h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>your airline is the biggest joke of an operat...</td>\n",
              "      <td>you r great salman siraap actor baad me ho peh...</td>\n",
              "      <td>0</td>\n",
              "      <td>your airline is the biggest joke of an operat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>what is going on with baggage claim in newark...</td>\n",
              "      <td>in suwro  ko \\nkonkon bich choraye pr latkana ...</td>\n",
              "      <td>1</td>\n",
              "      <td>what is going on with baggage claim in newark...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>what is going on with baggage claim in newark...</td>\n",
              "      <td>salman ji aapne iss bar koi song nahi gaya muj...</td>\n",
              "      <td>0</td>\n",
              "      <td>what is going on with baggage claim in newark...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>usually your lounge staff are fantastic excep...</td>\n",
              "      <td>ek madhar chod ravish kumar ab ise justify krn...</td>\n",
              "      <td>1</td>\n",
              "      <td>usually your lounge staff are fantastic excep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>usually your lounge staff are fantastic excep...</td>\n",
              "      <td>nice salman ji</td>\n",
              "      <td>0</td>\n",
              "      <td>usually your lounge staff are fantastic excep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text1  ...                                               text\n",
              "0      plus you've added commercials to the experien...  ...   plus you've added commercials to the experien...\n",
              "1      plus you've added commercials to the experien...  ...   plus you've added commercials to the experien...\n",
              "2      yes nearly every time i fly vx this ear worm ...  ...   yes nearly every time i fly vx this ear worm ...\n",
              "3      yes nearly every time i fly vx this ear worm ...  ...   yes nearly every time i fly vx this ear worm ...\n",
              "4                         well i didn'tbut now i do :-d  ...   well i didn'tbut now i do :-d aaj dil khush h...\n",
              "...                                                 ...  ...                                                ...\n",
              "7995   your airline is the biggest joke of an operat...  ...   your airline is the biggest joke of an operat...\n",
              "7996   what is going on with baggage claim in newark...  ...   what is going on with baggage claim in newark...\n",
              "7997   what is going on with baggage claim in newark...  ...   what is going on with baggage claim in newark...\n",
              "7998   usually your lounge staff are fantastic excep...  ...   usually your lounge staff are fantastic excep...\n",
              "7999   usually your lounge staff are fantastic excep...  ...   usually your lounge staff are fantastic excep...\n",
              "\n",
              "[8000 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocTjPBeGySOy",
        "outputId": "fa2aca55-8410-46b9-bf17-e86ec671a924"
      },
      "source": [
        "embed_text = new_df['text'].values # texts contrain all setences\n",
        "embed_text = [s.lower() for s in embed_text] # convert to lower case \n",
        "print(len(embed_text))\n",
        "print(embed_text[:2])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "[\" plus you've added commercials to the experience tacky modi g ke jajbe ko koti koti naman\", \" plus you've added commercials to the experience tacky do chutiya ek saath great moment\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hhHIG25pOwV"
      },
      "source": [
        "#Creating the vocabulary from the join sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWMYKMXnyows"
      },
      "source": [
        "# import string\n",
        "import re\n",
        "for i in range(len(embed_text)):\n",
        "  embed_text[i] = re.sub(r'[^a-z0-9\\s]', '', embed_text[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKEh6zhnywlS",
        "outputId": "389f535f-40fe-4795-e55a-a96c17d2d1a2"
      },
      "source": [
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(embed_text)\n",
        "tk.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 29,\n",
              " ' ': 2,\n",
              " 'UNK': 1,\n",
              " 'a': 3,\n",
              " 'b': 19,\n",
              " 'c': 20,\n",
              " 'd': 14,\n",
              " 'e': 4,\n",
              " 'f': 22,\n",
              " 'g': 18,\n",
              " 'h': 8,\n",
              " 'i': 5,\n",
              " 'j': 25,\n",
              " 'k': 15,\n",
              " 'l': 12,\n",
              " 'm': 16,\n",
              " 'n': 9,\n",
              " 'o': 7,\n",
              " 'p': 21,\n",
              " 'q': 28,\n",
              " 'r': 10,\n",
              " 's': 11,\n",
              " 't': 6,\n",
              " 'u': 13,\n",
              " 'v': 24,\n",
              " 'w': 23,\n",
              " 'x': 27,\n",
              " 'y': 17,\n",
              " 'z': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXGzZf7Dy39p",
        "outputId": "ec89f3ac-45a7-48c5-cf0f-0fc8efbfc520"
      },
      "source": [
        "alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "char_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 27,\n",
              " '1': 28,\n",
              " '2': 29,\n",
              " '3': 30,\n",
              " '4': 31,\n",
              " '5': 32,\n",
              " '6': 33,\n",
              " '7': 34,\n",
              " '8': 35,\n",
              " '9': 36,\n",
              " 'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8OGOLXHy7cP"
      },
      "source": [
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict \n",
        "# Add 'UNK' to the vocabulary \n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnHsCH77y-bm",
        "outputId": "24d64f8a-c2e0-4291-d14b-dba61b98f1f4"
      },
      "source": [
        "tk.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 27,\n",
              " '1': 28,\n",
              " '2': 29,\n",
              " '3': 30,\n",
              " '4': 31,\n",
              " '5': 32,\n",
              " '6': 33,\n",
              " '7': 34,\n",
              " '8': 35,\n",
              " '9': 36,\n",
              " 'UNK': 37,\n",
              " 'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P05A_M-3pwBa"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7-CU3JJFvO"
      },
      "source": [
        "X_train['text1'] = X_train['text1'].astype(str)\n",
        "X_train['text2'] = X_train['text2'].astype(str)\n",
        "X_val['text1'] = X_val['text1'].astype(str)\n",
        "X_val['text2'] = X_val['text2'].astype(str)\n",
        "X_test['text2'] = X_test['text2'].astype(str)\n",
        "X_test['text1'] = X_test['text1'].astype(str)\n",
        "\n",
        "train_q1_seq = tk.texts_to_sequences(X_train['text1'].values)\n",
        "train_q2_seq = tk.texts_to_sequences(X_train['text2'].values)\n",
        "val_q1_seq = tk.texts_to_sequences(X_val['text1'].values)\n",
        "val_q2_seq = tk.texts_to_sequences(X_val['text2'].values)\n",
        "test_q1_seq = tk.texts_to_sequences(X_test['text1'].values)\n",
        "test_q2_seq = tk.texts_to_sequences(X_test['text2'].values)\n",
        "\n",
        "max_len = 200\n",
        "train_q1_seq = pad_sequences(train_q1_seq, maxlen=max_len, padding='post')\n",
        "train_q2_seq = pad_sequences(train_q2_seq, maxlen=max_len, padding='post')\n",
        "test_q1_seq = pad_sequences(test_q1_seq, maxlen=max_len, padding='post')\n",
        "test_q2_seq = pad_sequences(test_q2_seq, maxlen=max_len, padding='post')\n",
        "val_q1_seq = pad_sequences(val_q1_seq, maxlen=max_len, padding='post')\n",
        "val_q2_seq = pad_sequences(val_q2_seq, maxlen=max_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhvT2HxTqULS"
      },
      "source": [
        "#Forming Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBKqV-S71L4G"
      },
      "source": [
        "vocab_size = len(tk.word_index)\n",
        "embedding_layer = Embedding(vocab_size + 1, 128, \n",
        "                            input_length=train_q1_seq.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Hf-hQqkgqq"
      },
      "source": [
        "Def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlNV_nsFqc4s"
      },
      "source": [
        "#Defining functions for Euclidean distance and contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1nv8zK_H5bt"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "    # unpack the vectors into separate lists\n",
        "    (featsA, featsB) = vectors\n",
        "    # compute the sum of squared distances between the vectors\n",
        "    sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n",
        "    # return the euclidean distance between the vectors\n",
        "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "def contrastive_loss(y, preds, margin=1):\n",
        "    # explicitly cast the true class label data type to the predicted\n",
        "    # class label data type (otherwise we run the risk of having two\n",
        "    # separate data types, causing TensorFlow to error out)\n",
        "    y = tf.cast(y, preds.dtype)\n",
        "    # calculate the contrastive loss between the true labels and\n",
        "    # the predicted labels\n",
        "    squaredPreds = K.square(preds)\n",
        "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
        "    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
        "    # return the computed contrastive loss to the calling function\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPYEsTrEqk0A"
      },
      "source": [
        "#Defining function for our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugp0W0FaqvHL"
      },
      "source": [
        "##Model 1.0: LSTM + 2 dense layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QI8nTHbH7PW"
      },
      "source": [
        "def build_network():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=38, output_dim=128, input_length=200))\n",
        "  model.add(Conv1D(filters=128,\n",
        "            kernel_size=3,\n",
        "            padding='valid',\n",
        "            activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #network.add(Embedding(name=\"synopsis_embedd\",input_dim =len(tk.word_index)+1, \n",
        "                       #output_dim=len(embeddings_index['no']),weights=[embedding_matrix], \n",
        "                       #input_length=train_q1_seq.shape[1],trainable=False))\n",
        "  model.add(LSTM(64,return_sequences=True, activation=\"relu\"))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu',\n",
        "                  kernel_regularizer=l2(1e-3),\n",
        "                  kernel_initializer='he_uniform'))\n",
        "  \n",
        "  model.add(Dense(2, activation=None,\n",
        "                  kernel_regularizer=l2(1e-3),\n",
        "                  kernel_initializer='he_uniform'))\n",
        "  \n",
        "  #Force the encoding to live on the d-dimentional hypershpere\n",
        "  model.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCnciMrPrRwo"
      },
      "source": [
        "##Model 2.0: Bi-LSTM + 1 dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOFQYa1abAg4"
      },
      "source": [
        "def build_network_2():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=38, output_dim=128, input_length=200))\n",
        "  model.add(Conv1D(filters=128,\n",
        "            kernel_size=3,\n",
        "            padding='valid',\n",
        "            activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #network.add(Embedding(name=\"synopsis_embedd\",input_dim =len(tk.word_index)+1, \n",
        "                       #output_dim=len(embeddings_index['no']),weights=[embedding_matrix], \n",
        "                       #input_length=train_q1_seq.shape[1],trainable=False))\n",
        "  model.add(Bidirectional(LSTM(64,return_sequences=True, activation=\"relu\")))\n",
        "  model.add(Flatten())  \n",
        "  model.add(Dense(2, activation=None,\n",
        "                  kernel_regularizer=l2(1e-3),\n",
        "                  kernel_initializer='he_uniform'))\n",
        "  \n",
        "  #Force the encoding to live on the d-dimentional hypershpere\n",
        "  model.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_h9B3MLrlss"
      },
      "source": [
        "##Model 3.0: Two LSTMs (Joshi implementation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poIjvIUhrO3g"
      },
      "source": [
        "def build_network_3():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=38, output_dim=128, input_length=200))\n",
        "  model.add(Conv1D(filters=128,\n",
        "            kernel_size=3,\n",
        "            padding='valid',\n",
        "            activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(128, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))\n",
        "  model.add(LSTM(128, recurrent_dropout=0.2, dropout=0.2, return_sequences=False))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "depV0R_8sTJ-"
      },
      "source": [
        "#Taking inputs from the dataset and passing to the model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6CGo-kyfk2G"
      },
      "source": [
        "input_1 = Input(shape=(train_q1_seq.shape[1],))\n",
        "input_2 = Input(shape=(train_q2_seq.shape[1],))\n",
        "\n",
        "test_input_1 = Input(shape=(test_q1_seq.shape[1],))\n",
        "test_input_2 = Input(shape=(test_q2_seq.shape[1],))\n",
        "\n",
        "network = build_network()\n",
        "\n",
        "encoded_input_1 = network(input_1)\n",
        "encoded_input_2 = network(input_2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8lle5nskZq6"
      },
      "source": [
        "##Starting the training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRKtG4b2hjd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca25239-98bd-478d-fa34-e08ea00b8320"
      },
      "source": [
        "distance = Lambda(euclidean_distance)([encoded_input_1, encoded_input_2])\n",
        "# Connect the inputs with the outputs\n",
        "model = Model([input_1, input_2], distance)\n",
        "model.compile(loss=contrastive_loss, optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_val = np.asarray(y_val).astype('float32')\n",
        "y_test = np.asarray(y_test).astype('float32')\n",
        "model.fit([train_q1_seq,train_q2_seq],y_train.reshape(-1,1), epochs = 8, \n",
        "          batch_size=64,validation_data=([val_q1_seq, val_q2_seq],y_val.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "80/80 [==============================] - 36s 415ms/step - loss: 0.4911 - accuracy: 0.4756 - val_loss: 0.4235 - val_accuracy: 0.4375\n",
            "Epoch 2/8\n",
            "80/80 [==============================] - 33s 411ms/step - loss: 0.4202 - accuracy: 0.4678 - val_loss: 0.3903 - val_accuracy: 0.4430\n",
            "Epoch 3/8\n",
            "80/80 [==============================] - 33s 411ms/step - loss: 0.3920 - accuracy: 0.4695 - val_loss: 0.3662 - val_accuracy: 0.4406\n",
            "Epoch 4/8\n",
            "80/80 [==============================] - 33s 411ms/step - loss: 0.3704 - accuracy: 0.4656 - val_loss: 0.3602 - val_accuracy: 0.4711\n",
            "Epoch 5/8\n",
            "80/80 [==============================] - 33s 409ms/step - loss: 0.3571 - accuracy: 0.4656 - val_loss: 0.3418 - val_accuracy: 0.4523\n",
            "Epoch 6/8\n",
            "80/80 [==============================] - 33s 411ms/step - loss: 0.3449 - accuracy: 0.4656 - val_loss: 0.3359 - val_accuracy: 0.4578\n",
            "Epoch 7/8\n",
            "80/80 [==============================] - 33s 409ms/step - loss: 0.3337 - accuracy: 0.4584 - val_loss: 0.3596 - val_accuracy: 0.4633\n",
            "Epoch 8/8\n",
            "80/80 [==============================] - 33s 408ms/step - loss: 0.3241 - accuracy: 0.4471 - val_loss: 0.3229 - val_accuracy: 0.4547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe8582db390>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nysC9jtatcvJ"
      },
      "source": [
        "#Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUIBwqPcuaTU"
      },
      "source": [
        "# Save model for further use\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"siamesemodel-contrastive-loss.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "#serialize weights to HDF5\n",
        "model.save_weights(\"siamesemodel-contrastive-loss.h5\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3xh-gnitlwR"
      },
      "source": [
        "#Loading the same model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGXBV7GBysH7"
      },
      "source": [
        "json_file = open('siamesemodel-contrastive-loss.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(\"siamesemodel-contrastive-loss.h5\")\n",
        "# loaded_model.compile(loss=contrastive_loss, optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "# loaded_model.fit([train_q1_seq,train_q2_seq],y_train.reshape(-1,1), epochs = 5, \n",
        "# batch_size=64,validation_data=([val_q1_seq, val_q2_seq],y_val.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmPYI-H3t27D"
      },
      "source": [
        "#Prediction phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYr2z9I4t6a2"
      },
      "source": [
        "##Predicting for a positive using a positive assistant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt3ucU5-RMwZ",
        "outputId": "6db6cb92-ac13-42a3-a691-627931d4b335"
      },
      "source": [
        "prediction_data = \"Good deed modi bhai ji......hme hmara desh fir se golden sparrow bnana h....wo spna sch hoga...jiske liye god n apko select kiya h...thnku god\"\n",
        "prediction_vector = tk.texts_to_sequences([prediction_data])\n",
        "prediction_vector = pad_sequences(prediction_vector,maxlen=200)\n",
        "\n",
        "assistant_data = \"masha allah good....yeh hain meri hindi\"\n",
        "assistant_vector = tk.texts_to_sequences([assistant_data])\n",
        "assistant_vector = pad_sequences(assistant_vector,maxlen=200)\n",
        "loaded_model.predict([prediction_vector, assistant_vector])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92679805]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vXz_7MGuR5d"
      },
      "source": [
        "##Predicting for a negative using a positive assistant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCiqn_AluMEj",
        "outputId": "a6c345c0-3205-40b0-ab50-5b61d17feab2"
      },
      "source": [
        "prediction_data = \"Kyo modi ji bihar ke yua berojgar hi rhenge. Bihar ki gandi rajniti me sare yua piss rhe h\"\n",
        "prediction_vector = tk.texts_to_sequences([prediction_data])\n",
        "prediction_vector = pad_sequences(prediction_vector,maxlen=200)\n",
        "\n",
        "assistant_data = \"Amazing respect for modi even  years back salute to you Amit sir aap logo ki jai\"\n",
        "assistant_vector = tk.texts_to_sequences([assistant_data])\n",
        "assistant_vector = pad_sequences(assistant_vector,maxlen=200)\n",
        "\n",
        "loaded_model.predict([prediction_vector, assistant_vector])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04976079]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E11m-EQwuYhO"
      },
      "source": [
        "#Evalutaing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlvXhB6aWeAr",
        "outputId": "56906102-bed7-44a9-da17-7b60e6e98f2e"
      },
      "source": [
        "results = model.evaluate([test_q1_seq,test_q2_seq], y_test.reshape(-1,1), batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 3s 192ms/step - loss: 0.3395 - accuracy: 0.4769\n",
            "test loss, test acc: [0.33949315547943115, 0.4768750071525574]\n"
          ]
        }
      ]
    }
  ]
}